from __future__ import annotations

import os
from datetime import datetime, timedelta
from threading import Thread
from typing import Any, Dict, List, Optional, Type, TypeVar
from uuid import uuid4

import yaml
from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, Request, status
from fastapi.responses import StreamingResponse
from sqlalchemy import func
from sqlmodel import select

from db.models import (
    Artifact,
    HostedFeed,
    Indicator,
    IndicatorType,
    PipelineRun,
    Profile,
    ProfileConfig,
    RunError,
    RunState,
    AugmentedIndicator,
)
from db.persistence import (
    create_pipeline_run_record,
    create_profile,
    create_profile_config,
    finalize_pipeline_run,
    record_run_error,
    update_run_state,
)
from db.session import ensure_schema, get_session, session_scope
from pipeline.engine import EngineConfig, run_pipeline
from utils.logger import get_logger

from .auth import require_api_key
from .jobs import JobStore
from .refresh import RefreshScheduler
from pydantic import BaseModel, ValidationError
from .schemas import (
    ProfileCreateRequest,
    ProfileResponse,
    ProfileSummary,
    ProfileConfigCreateRequest,
    ProfileConfigResponse,
    ProfileConfigSummary,
    RunCreateRequest,
    RunSubmissionResponse,
    RunListResponse,
    PipelineRunSummary,
    PipelineRunDetail,
    ArtifactResponse,
    RunErrorResponse,
    JobStatusResponse,
)


app = FastAPI(
    title="EDL Pipeline API",
    version="2.0.0",
    description=(
        "API surface for orchestrating the EDL ingestion/validation pipeline. "
        "Manage profiles, versioned configurations, and track pipeline runs."
    ),
)

job_store = JobStore()
refresh_scheduler = RefreshScheduler()
api_logger = get_logger("api.app", "INFO", "api.log")

PayloadModel = TypeVar("PayloadModel", bound=BaseModel)

HOSTED_TYPES = {
    "url": IndicatorType.URL,
    "fqdn": IndicatorType.FQDN,
    "ipv4": IndicatorType.IPV4,
    "ipv6": IndicatorType.IPV6,
    "cidr": IndicatorType.CIDR,
}


# ---------------------------------------------------------------------------
# Startup / shutdown
# ---------------------------------------------------------------------------
@app.on_event("startup")
def startup_event() -> None:
    ensure_schema()
    interval_env = os.getenv("EDL_AUTO_REFRESH_MINUTES")
    if interval_env:
        try:
            interval = float(interval_env)
        except ValueError:
            api_logger.warning("Invalid EDL_AUTO_REFRESH_MINUTES value '%s'; skipping scheduler.", interval_env)
            interval = 0.0
        if interval > 0:
            refresh_scheduler.start(interval, _auto_refresh_tick)


@app.on_event("shutdown")
def shutdown_event() -> None:
    refresh_scheduler.stop()


# ---------------------------------------------------------------------------
# Health check
# ---------------------------------------------------------------------------
@app.get("/health", tags=["system"])
def healthcheck() -> Dict[str, str]:
    return {"status": "ok"}


# ---------------------------------------------------------------------------
# Profile management
# ---------------------------------------------------------------------------
@app.post(
    "/profiles",
    response_model=ProfileResponse,
    status_code=status.HTTP_201_CREATED,
    tags=["profiles"],
    dependencies=[Depends(require_api_key)],
)
async def create_profile_endpoint(request: Request) -> ProfileResponse:
    body = await _safe_json(request)
    payload = _extract_payload(body, ProfileCreateRequest)
    profile = create_profile(name=payload.name, description=payload.description)
    return _serialize_profile(profile)


@app.get("/profiles", response_model=List[ProfileSummary], tags=["profiles"])
def list_profiles_endpoint() -> List[ProfileSummary]:
    session = get_session()
    try:
        profiles = session.exec(select(Profile)).all()
        results: List[ProfileSummary] = []
        for profile in profiles:
            latest_version = session.exec(
                select(func.max(ProfileConfig.version)).where(ProfileConfig.profile_id == profile.id)
            ).one()
            total_runs = session.exec(
                select(func.count()).select_from(PipelineRun).where(PipelineRun.profile_id == profile.id)
            ).one()
            active_runs = session.exec(
                select(func.count()).select_from(PipelineRun).where(
                    PipelineRun.profile_id == profile.id,
                    PipelineRun.state.in_([RunState.QUEUED, RunState.RUNNING]),
                )
            ).one()
            results.append(
                ProfileSummary(
                    id=profile.id,
                    name=profile.name,
                    description=profile.description,
                    created_at=profile.created_at,
                    updated_at=profile.updated_at,
                    latest_config_version=int(latest_version or 0) or None,
                    total_runs=int(total_runs or 0),
                    active_runs=int(active_runs or 0),
                )
            )
        return results
    finally:
        session.close()


@app.post(
    "/profiles/{profile_id}/configs",
    response_model=ProfileConfigResponse,
    status_code=status.HTTP_201_CREATED,
    tags=["profiles"],
    dependencies=[Depends(require_api_key)],
)
async def create_profile_config_endpoint(
    profile_id: str, request: Request
) -> ProfileConfigResponse:
    body = await _safe_json(request)
    payload = _extract_payload(body, ProfileConfigCreateRequest)
    try:
        config = create_profile_config(
            profile_id=profile_id,
            sources_yaml=payload.sources_yaml,
            augment_yaml=payload.augment_yaml,
            pipeline_settings=payload.pipeline_settings,
            created_by=payload.created_by,
        )
    except ValueError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc

    return ProfileConfigResponse(
        id=config.id,
        profile_id=config.profile_id,
        version=config.version,
        config_hash=config.config_hash,
        created_at=config.created_at,
        created_by=config.created_by,
        pipeline_settings=config.pipeline_settings,
    )


@app.get(
    "/profiles/{profile_id}/configs",
    response_model=List[ProfileConfigSummary],
    tags=["profiles"],
)
def list_profile_configs_endpoint(profile_id: str) -> List[ProfileConfigSummary]:
    session = get_session()
    try:
        configs = session.exec(
            select(ProfileConfig).where(ProfileConfig.profile_id == profile_id).order_by(ProfileConfig.version.desc())
        ).all()
        if not configs:
            _ensure_profile_exists(session, profile_id)
        return [
            ProfileConfigSummary(
                id=config.id,
                profile_id=config.profile_id,
                version=config.version,
                created_at=config.created_at,
                created_by=config.created_by,
                pipeline_settings=config.pipeline_settings,
            )
            for config in configs
        ]
    finally:
        session.close()


# ---------------------------------------------------------------------------
# Run management
# ---------------------------------------------------------------------------
@app.post(
    "/runs",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=RunSubmissionResponse,
    tags=["runs"],
    dependencies=[Depends(require_api_key)],
)
async def create_run_endpoint(
    background_tasks: BackgroundTasks, request: Request
) -> RunSubmissionResponse:
    body = await _safe_json(request)
    payload = _extract_payload(body, RunCreateRequest)
    session = get_session()
    try:
        profile = session.get(Profile, payload.profile_id)
        if not profile:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Profile not found")
        if payload.profile_config_id:
            config = session.get(ProfileConfig, payload.profile_config_id)
            if not config or config.profile_id != profile.id:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Profile configuration not found")
        else:
            config = session.exec(
                select(ProfileConfig).where(ProfileConfig.profile_id == profile.id).order_by(ProfileConfig.version.desc())
            ).first()
            if not config:
                raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Profile has no configurations")
        session.expunge(config)
        session.expunge(profile)
    finally:
        session.close()

    overrides = dict(payload.overrides or {})

    return _enqueue_pipeline_run(
        profile=profile,
        config=config,
        overrides=overrides,
        background_tasks=background_tasks,
    )


@app.get("/runs", response_model=RunListResponse, tags=["runs"])
def list_runs_endpoint(
    state: Optional[RunState] = None,
    profile_id: Optional[str] = None,
) -> RunListResponse:
    session = get_session()
    try:
        query = select(PipelineRun)
        if state:
            query = query.where(PipelineRun.state == state)
        if profile_id:
            query = query.where(PipelineRun.profile_id == profile_id)
        query = query.order_by(PipelineRun.queued_at.desc())
        runs = session.exec(query).all()
        summaries = [_serialize_run_summary(run) for run in runs]
        return RunListResponse(runs=summaries)
    finally:
        session.close()


@app.get("/runs/{run_id}", response_model=PipelineRunDetail, tags=["runs"])
def get_run_detail_endpoint(run_id: str) -> PipelineRunDetail:
    session = get_session()
    try:
        run = session.get(PipelineRun, run_id)
        if not run:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Run not found")
        artifacts = session.exec(select(Artifact).where(Artifact.run_id == run.id)).all()
        errors = session.exec(select(RunError).where(RunError.run_id == run.id)).all()
        return _serialize_run_detail(run, artifacts, errors)
    finally:
        session.close()


@app.get("/jobs", response_model=List[JobStatusResponse], tags=["runs"])
def list_jobs_endpoint() -> List[JobStatusResponse]:
    responses: List[JobStatusResponse] = []
    runs_cache: Dict[str, PipelineRunSummary] = {}
    for job_id, record in job_store.all().items():
        summary = None
        if record.run_id:
            summary = runs_cache.get(record.run_id)
            if not summary:
                summary = _get_run_summary(record.run_id)
                if summary:
                    runs_cache[record.run_id] = summary
        responses.append(
            JobStatusResponse(
                job_id=job_id,
                run_id=record.run_id,
                profile_id=record.profile_id,
                profile_config_id=record.profile_config_id,
                state=record.state,
                created_at=record.created_at,
                updated_at=record.updated_at,
                detail=record.detail,
            )
        )
    return responses


# ---------------------------------------------------------------------------
# Hosted feed endpoints
# ---------------------------------------------------------------------------
@app.get(
    "/edl/{indicator_type}",
    response_class=StreamingResponse,
    tags=["feeds"],
    summary="Hosted EDL output segregated by indicator type.",
)
def serve_edl(indicator_type: str) -> StreamingResponse:
    indicator_key = indicator_type.lower()
    if indicator_key not in HOSTED_TYPES:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Unsupported indicator type.")

    indicator_enum = HOSTED_TYPES[indicator_key]
    run_id = _resolve_hosted_run(indicator_enum)

    if not run_id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="No pipeline runs available.")

    def stream(run_id: str):
        session = get_session()
        try:
            query = (
                select(Indicator.normalized, AugmentedIndicator.augmented_value)
                .select_from(Indicator)
                .outerjoin(AugmentedIndicator, AugmentedIndicator.indicator_id == Indicator.id)
                .where(
                    Indicator.run_id == run_id,
                    Indicator.entry_type == indicator_enum,
                    Indicator.is_valid == True,  # noqa: E712
                )
                .order_by(Indicator.normalized)
            )
            result = session.exec(query)
            for normalized, augmented_value in result:
                value = augmented_value or normalized
                if value:
                    yield f"{value}\n"
        finally:
            session.close()

    headers = {
        "Cache-Control": "no-cache",
        "X-EDL-Run-ID": run_id,
        "Content-Disposition": f'inline; filename="{indicator_key}.txt"',
    }
    return StreamingResponse(stream(run_id), media_type="text/plain", headers=headers)


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------
def _execute_pipeline_job(job_id: str) -> None:
    record = job_store.get(job_id)
    if not record:
        return

    try:
        job_store.update(job_id, state=RunState.RUNNING, detail="Pipeline execution started")
        update_run_state(
            record.run_id,
            state=RunState.RUNNING,
            sub_state="fetch",
            percent_complete=0.0,
            metadata={"job_id": job_id},
            started_at=datetime.utcnow(),
        )

        session = get_session()
        try:
            config = session.get(ProfileConfig, record.profile_config_id) if record.profile_config_id else None
            if not config:
                raise RuntimeError("Profile configuration not found")
            sources_yaml = config.sources_yaml
            augment_yaml = config.augment_yaml
            pipeline_settings = dict(config.pipeline_settings or {})
        finally:
            session.close()

        overrides = record.request.get("overrides") or {}
        pipeline_settings.update(overrides)

        sources = EngineConfig.parse_sources_yaml(sources_yaml, api_logger)
        augmentor_cfg = EngineConfig.parse_augmentor_yaml(augment_yaml, api_logger)

        mode = pipeline_settings.get("mode", "validate")
        timeout = int(pipeline_settings.get("timeout", 15))
        log_level = pipeline_settings.get("log_level", "INFO")
        output_path = pipeline_settings.get("output_path", "test_output_data/validated_output.json")
        persist_flag = bool(pipeline_settings.get("persist_to_db", True))

        run_pipeline(
            sources=sources,
            output=output_path,
            mode=mode,
            augmentor_cfg=augmentor_cfg if mode == "augment" else None,
            timeout=timeout,
            log_level=log_level,
            persist_to_db=persist_flag,
            profile_id=record.profile_id,
            profile_config_id=record.profile_config_id,
            run_id=record.run_id,
        )

        _update_hosted_feeds(record.run_id)

        job_store.update(job_id, state=RunState.SUCCESS, detail="Pipeline completed successfully")

        if record.profile_id:
            with session_scope() as session:
                profile = session.get(Profile, record.profile_id)
                if profile:
                    profile.last_refreshed_at = datetime.utcnow()
                    session.add(profile)

    except Exception as exc:  # noqa: BLE001
        api_logger.exception("Pipeline job %s failed", job_id)
        if record.run_id:
            record_run_error(
                run_id=record.run_id,
                phase="pipeline",
                source=None,
                message=str(exc),
            )
            update_run_state(
                record.run_id,
                state=RunState.FAILED,
                sub_state=None,
                percent_complete=100.0,
            )
        job_store.update(job_id, state=RunState.FAILED, detail=str(exc), error=str(exc))


def _update_hosted_feeds(run_id: str) -> None:
    session = get_session()
    try:
        run = session.get(PipelineRun, run_id)
        if not run:
            return
        counts = session.exec(
            select(Indicator.entry_type, func.count()).where(Indicator.run_id == run.id, Indicator.is_valid == True).group_by(Indicator.entry_type)  # noqa: E712
        ).all()
        count_map = {entry_type.value: total for entry_type, total in counts}

        for key, indicator_enum in HOSTED_TYPES.items():
            if count_map.get(key):
                hosted = session.exec(
                    select(HostedFeed).where(HostedFeed.indicator_type == indicator_enum)
                ).one_or_none()
                if hosted:
                    hosted.run_id = run.id
                    hosted.updated_at = datetime.utcnow()
                    session.add(hosted)
                else:
                    session.add(HostedFeed(indicator_type=indicator_enum, run_id=run.id))
        session.commit()
    finally:
        session.close()


def _resolve_hosted_run(indicator_enum: IndicatorType) -> Optional[str]:
    session = get_session()
    try:
        hosted_run_id = session.exec(
            select(HostedFeed.run_id).where(HostedFeed.indicator_type == indicator_enum)
        ).one_or_none()
        if hosted_run_id:
            return hosted_run_id
        return session.exec(select(PipelineRun.id).order_by(PipelineRun.queued_at.desc())).first()
    finally:
        session.close()


def _ensure_profile_exists(session, profile_id: str) -> None:
    profile = session.get(Profile, profile_id)
    if not profile:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Profile not found")


def _enqueue_pipeline_run(
    *,
    profile: Profile,
    config: ProfileConfig,
    overrides: Dict[str, Any],
    background_tasks: Optional[BackgroundTasks],
) -> RunSubmissionResponse:
    settings = dict(config.pipeline_settings or {})
    settings.update(overrides)

    mode = settings.get("mode", "validate")

    run_id = create_pipeline_run_record(
        profile_id=profile.id,
        profile_config_id=config.id,
        mode=mode,
        metadata={"requested_overrides": overrides},
    )

    job_id = str(uuid4())
    job_store.create_job(
        job_id=job_id,
        request={"overrides": overrides},
        profile_id=profile.id,
        profile_config_id=config.id,
    )
    job_store.update(job_id, run_id=run_id, detail="Queued pipeline run")

    if background_tasks is not None:
        background_tasks.add_task(_execute_pipeline_job, job_id)
    else:
        Thread(target=_execute_pipeline_job, args=(job_id,), daemon=True).start()

    return RunSubmissionResponse(
        job_id=job_id,
        run_id=run_id,
        profile_id=profile.id,
        profile_config_id=config.id,
        state=RunState.QUEUED,
        detail=f"Pipeline run accepted. Track status via GET /runs/{run_id}",
    )


async def _safe_json(request: Request) -> Dict[str, Any]:
    try:
        data = await request.json()
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Request body must be valid JSON.",
        ) from exc
    if not isinstance(data, dict):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="JSON body must be an object.",
        )
    return data


def _extract_payload(body: Dict[str, Any], model: Type[PayloadModel]) -> PayloadModel:
    if not isinstance(body, dict):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Invalid JSON payload.",
        )

    if "payload" in body and isinstance(body["payload"], dict):
        candidate = body["payload"]
    elif "request" in body and isinstance(body["request"], dict):
        candidate = body["request"]
    else:
        candidate = body

    try:
        return model.model_validate(candidate)
    except ValidationError as exc:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=exc.errors(),
        ) from exc


def _serialize_profile(profile: Profile) -> ProfileResponse:
    return ProfileResponse(
        id=profile.id,
        name=profile.name,
        description=profile.description,
        created_at=profile.created_at,
        updated_at=profile.updated_at,
        latest_config_version=None,
    )


def _serialize_run_summary(run: PipelineRun) -> PipelineRunSummary:
    return PipelineRunSummary(
        run_id=run.id,
        profile_id=run.profile_id,
        profile_config_id=run.profile_config_id,
        mode=run.mode,
        state=run.state,
        sub_state=run.sub_state,
        percent_complete=run.percent_complete,
        queued_at=run.queued_at,
        started_at=run.started_at,
        completed_at=run.completed_at,
        total_fetched=run.total_fetched,
        total_ingested=run.total_ingested,
        total_valid=run.total_valid,
        total_invalid=run.total_invalid,
        total_augmented=run.total_augmented,
    )


def _serialize_run_detail(
    run: PipelineRun,
    artifacts: List[Artifact],
    errors: List[RunError],
) -> PipelineRunDetail:
    return PipelineRunDetail(
        **_serialize_run_summary(run).model_dump(),
        metadata_snapshot=run.metadata_snapshot or {},
        artifacts=[_artifact_to_response(a) for a in artifacts],
        errors=[_error_to_response(e) for e in errors],
    )


def _artifact_to_response(artifact: Artifact) -> ArtifactResponse:
    return ArtifactResponse(
        id=artifact.id,
        run_id=artifact.run_id,
        artifact_type=artifact.artifact_type,
        location=artifact.location,
        status=artifact.status,
        checksum=artifact.checksum,
        size_bytes=artifact.size_bytes,
        created_at=artifact.created_at,
        updated_at=artifact.updated_at,
    )


def _error_to_response(error: RunError) -> RunErrorResponse:
    return RunErrorResponse(
        id=error.id,
        phase=error.phase,
        source=error.source,
        message=error.message,
        detail=error.detail,
        created_at=error.created_at,
    )


def _get_run_summary(run_id: str) -> Optional[PipelineRunSummary]:
    session = get_session()
    try:
        run = session.get(PipelineRun, run_id)
        if not run:
            return None
        return _serialize_run_summary(run)
    finally:
        session.close()


def _auto_refresh_tick() -> None:
    session = get_session()
    try:
        profiles = session.exec(
            select(Profile).where(
                Profile.refresh_interval_minutes.is_not(None),
                Profile.refresh_interval_minutes > 0,
            )
        ).all()
    finally:
        session.close()

    now = datetime.utcnow()
    for profile in profiles or []:
        interval = profile.refresh_interval_minutes or 0.0
        if interval <= 0:
            continue
        if job_store.has_active_job_for_profile(profile.id):
            continue
        last = profile.last_refreshed_at or profile.updated_at or profile.created_at
        if last and (now - last) < timedelta(minutes=interval):
            continue

        api_logger.info(
            "Auto-refresh scheduling profile %s (interval %.2f minutes)",
            profile.id,
            interval,
        )

        background_tasks = BackgroundTasks()
        request = RunCreateRequest(profile_id=profile.id, profile_config_id=None, overrides={})
        create_run_endpoint(request, background_tasks)


__all__ = ["app"]
